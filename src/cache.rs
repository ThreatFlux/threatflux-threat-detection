use anyhow::Result;
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::fs;
use std::path::{Path, PathBuf};
use std::sync::Arc;
use tokio::sync::{RwLock, Semaphore};
use tokio::fs::File;
use tokio::io::AsyncWriteExt;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CacheEntry {
    pub file_path: String,
    pub file_hash: String,
    pub tool_name: String,
    pub tool_args: HashMap<String, serde_json::Value>,
    pub result: serde_json::Value,
    pub timestamp: DateTime<Utc>,
    pub file_size: u64,
    pub execution_time_ms: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CacheMetadata {
    pub total_entries: usize,
    pub total_unique_files: usize,
    pub cache_size_bytes: u64,
    pub last_updated: DateTime<Utc>,
}

#[derive(Clone)]
pub struct AnalysisCache {
    entries: Arc<RwLock<HashMap<String, Vec<CacheEntry>>>>,
    cache_dir: PathBuf,
    max_entries_per_file: usize,
    max_total_entries: usize,
    save_semaphore: Arc<Semaphore>,
}

impl AnalysisCache {
    pub fn new<P: AsRef<Path>>(cache_dir: P) -> Result<Self> {
        let cache_dir = cache_dir.as_ref().to_path_buf();

        // Create cache directory if it doesn't exist
        fs::create_dir_all(&cache_dir)?;

        let cache = Self {
            entries: Arc::new(RwLock::new(HashMap::new())),
            cache_dir,
            max_entries_per_file: 100,
            max_total_entries: 10000, // Global cache limit
            save_semaphore: Arc::new(Semaphore::new(1)), // Only one save at a time
        };

        // Load existing cache
        let cache_clone = cache.clone();
        tokio::spawn(async move {
            let _ = cache_clone.load_from_disk().await;
        });

        Ok(cache)
    }

    pub async fn add_entry(&self, entry: CacheEntry) -> Result<()> {
        {
            let mut entries = self.entries.write().await;

            // Use file hash as key for better cache hits
            let key = entry.file_hash.clone();

            let file_entries = entries.entry(key).or_insert_with(Vec::new);
            file_entries.push(entry);

            // Limit entries per file
            if file_entries.len() > self.max_entries_per_file {
                file_entries.remove(0);
            }

            // Enforce global cache limit with LRU eviction
            let total_entries: usize = entries.values().map(|v| v.len()).sum();
            if total_entries > self.max_total_entries {
                self.evict_oldest(&mut entries).await;
            }
        }

        // Batch save operations with semaphore
        let cache_clone = self.clone();
        tokio::spawn(async move {
            let _permit = cache_clone.save_semaphore.acquire().await.unwrap();
            let _ = cache_clone.save_to_disk().await;
        });

        Ok(())
    }

    // LRU eviction method to prevent unbounded growth
    async fn evict_oldest(&self, entries: &mut HashMap<String, Vec<CacheEntry>>) {
        // Find the oldest entry across all files
        let mut oldest_hash: Option<String> = None;
        let mut oldest_timestamp = chrono::Utc::now();
        
        for (hash, file_entries) in entries.iter() {
            if let Some(entry) = file_entries.first() {
                if entry.timestamp < oldest_timestamp {
                    oldest_timestamp = entry.timestamp;
                    oldest_hash = Some(hash.clone());
                }
            }
        }
        
        // Remove the oldest file's entries
        if let Some(hash) = oldest_hash {
            entries.remove(&hash);
        }
    }

    #[allow(dead_code)]
    pub async fn get_entries(&self, file_hash: &str) -> Option<Vec<CacheEntry>> {
        let entries = self.entries.read().await;
        entries.get(file_hash).cloned()
    }

    #[allow(dead_code)]
    pub async fn get_latest_analysis(&self, file_hash: &str, tool_name: &str) -> Option<CacheEntry> {
        let entries = self.entries.read().await;
        entries
            .get(file_hash)?
            .iter()
            .filter(|e| e.tool_name == tool_name)
            .max_by_key(|e| e.timestamp)
            .cloned()
    }

    pub async fn get_all_entries(&self) -> Vec<CacheEntry> {
        let entries = self.entries.read().await;
        entries.values().flat_map(|v| v.iter().cloned()).collect()
    }

    pub async fn get_metadata(&self) -> CacheMetadata {
        let entries = self.entries.read().await;

        let total_entries: usize = entries.values().map(|v| v.len()).sum();
        let total_unique_files = entries.len();

        // More efficient size calculation without serialization
        let cache_size_bytes: u64 = entries
            .values()
            .flat_map(|v| v.iter())
            .map(|e| {
                // Estimate based on string lengths instead of serialization
                (e.file_path.len() + e.file_hash.len() + e.tool_name.len() + 1024) as u64
            })
            .sum();

        CacheMetadata {
            total_entries,
            total_unique_files,
            cache_size_bytes,
            last_updated: Utc::now(),
        }
    }

    pub async fn clear(&self) -> Result<()> {
        let mut entries = self.entries.write().await;
        entries.clear();

        // Remove cache files
        if self.cache_dir.exists() {
            for entry in fs::read_dir(&self.cache_dir)? {
                let entry = entry?;
                if entry.path().extension().and_then(|s| s.to_str()) == Some("json") {
                    fs::remove_file(entry.path())?;
                }
            }
        }

        Ok(())
    }

    pub async fn search_entries(&self, query: &CacheSearchQuery) -> Vec<CacheEntry> {
        let entries = self.entries.read().await;

        entries
            .values()
            .flat_map(|v| v.iter())
            .filter(|e| {
                let mut matches = true;

                if let Some(ref tool) = query.tool_name {
                    matches &= e.tool_name == *tool;
                }

                if let Some(ref path_pattern) = query.file_path_pattern {
                    matches &= e.file_path.contains(path_pattern);
                }

                if let Some(ref start) = query.start_time {
                    matches &= e.timestamp >= *start;
                }

                if let Some(ref end) = query.end_time {
                    matches &= e.timestamp <= *end;
                }

                if let Some(min_size) = query.min_file_size {
                    matches &= e.file_size >= min_size;
                }

                if let Some(max_size) = query.max_file_size {
                    matches &= e.file_size <= max_size;
                }

                matches
            })
            .cloned()
            .collect()
    }

    async fn save_to_disk(&self) -> Result<()> {
        // Batch save operations to reduce I/O
        let entries = {
            let entries_guard = self.entries.read().await;
            entries_guard.clone()
        };

        // Only save recently modified entries (simplified approach)
        for (hash, file_entries) in entries.iter() {
            let cache_file = self.cache_dir.join(format!("{}.json", hash));
            let json = serde_json::to_string_pretty(file_entries)?;

            let mut file = File::create(&cache_file).await?;
            file.write_all(json.as_bytes()).await?;
            file.flush().await?;
        }

        // Save metadata
        let metadata = self.get_metadata().await;
        let metadata_file = self.cache_dir.join("metadata.json");
        let json = serde_json::to_string_pretty(&metadata)?;

        let mut file = File::create(&metadata_file).await?;
        file.write_all(json.as_bytes()).await?;
        file.flush().await?;

        Ok(())
    }

    async fn load_from_disk(&self) -> Result<()> {
        if !self.cache_dir.exists() {
            return Ok(());
        }

        let mut entries = self.entries.write().await;

        for entry in fs::read_dir(&self.cache_dir)? {
            let entry = entry?;
            let path = entry.path();

            if path.extension().and_then(|s| s.to_str()) == Some("json")
                && path.file_stem().and_then(|s| s.to_str()) != Some("metadata")
            {
                let json = fs::read_to_string(&path)?;
                if let Ok(file_entries) = serde_json::from_str::<Vec<CacheEntry>>(&json) {
                    if let Some(stem) = path.file_stem().and_then(|s| s.to_str()) {
                        entries.insert(stem.to_string(), file_entries);
                    }
                }
            }
        }

        Ok(())
    }

    pub async fn get_statistics(&self) -> CacheStatistics {
        let entries = self.entries.read().await;

        let mut tool_counts: HashMap<String, usize> = HashMap::new();
        let mut total_execution_time = 0u64;
        let mut file_type_counts: HashMap<String, usize> = HashMap::new();

        for file_entries in entries.values() {
            for entry in file_entries {
                *tool_counts.entry(entry.tool_name.clone()).or_insert(0) += 1;
                total_execution_time += entry.execution_time_ms;

                // Extract file extension
                if let Some(ext) = Path::new(&entry.file_path)
                    .extension()
                    .and_then(|s| s.to_str())
                {
                    *file_type_counts.entry(ext.to_string()).or_insert(0) += 1;
                }
            }
        }

        let total_entries: usize = entries.values().map(|v| v.len()).sum();
        let avg_execution_time = if total_entries > 0 {
            total_execution_time / total_entries as u64
        } else {
            0
        };

        CacheStatistics {
            tool_counts,
            file_type_counts,
            total_analyses: total_entries,
            unique_files: entries.len(),
            avg_execution_time_ms: avg_execution_time,
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CacheSearchQuery {
    pub tool_name: Option<String>,
    pub file_path_pattern: Option<String>,
    pub start_time: Option<DateTime<Utc>>,
    pub end_time: Option<DateTime<Utc>>,
    pub min_file_size: Option<u64>,
    pub max_file_size: Option<u64>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CacheStatistics {
    pub tool_counts: HashMap<String, usize>,
    pub file_type_counts: HashMap<String, usize>,
    pub total_analyses: usize,
    pub unique_files: usize,
    pub avg_execution_time_ms: u64,
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;
    use chrono::{Duration, Utc};

    fn create_test_entry(file_hash: &str, tool_name: &str, file_path: &str) -> CacheEntry {
        CacheEntry {
            file_path: file_path.to_string(),
            file_hash: file_hash.to_string(),
            tool_name: tool_name.to_string(),
            tool_args: [("test_arg".to_string(), serde_json::json!("value"))].iter().cloned().collect(),
            result: serde_json::json!({"result": "test"}),
            timestamp: Utc::now(),
            file_size: 1024,
            execution_time_ms: 100,
        }
    }

    #[tokio::test]
    async fn test_cache_creation() {
        let temp_dir = TempDir::new().unwrap();
        let cache = AnalysisCache::new(temp_dir.path()).unwrap();

        let metadata = cache.get_metadata();
        assert_eq!(metadata.total_entries, 0);
        assert_eq!(metadata.total_unique_files, 0);
    }

    #[tokio::test]
    async fn test_add_and_get_entry() {
        let temp_dir = TempDir::new().unwrap();
        let cache = AnalysisCache::new(temp_dir.path()).unwrap();

        let entry = create_test_entry("hash123", "test_tool", "/test/file.bin");
        cache.add_entry(entry.clone()).unwrap();

        let entries = cache.get_entries("hash123").unwrap();
        assert_eq!(entries.len(), 1);
        assert_eq!(entries[0].tool_name, "test_tool");
        assert_eq!(entries[0].file_path, "/test/file.bin");
        assert_eq!(entries[0].file_hash, "hash123");
        assert_eq!(entries[0].file_size, 1024);
        assert_eq!(entries[0].execution_time_ms, 100);
    }

    #[tokio::test]
    async fn test_multiple_entries_same_file() {
        let temp_dir = TempDir::new().unwrap();
        let cache = AnalysisCache::new(temp_dir.path()).unwrap();

        let entry1 = create_test_entry("hash123", "tool1", "/test/file.bin");
        let entry2 = create_test_entry("hash123", "tool2", "/test/file.bin");

        cache.add_entry(entry1).unwrap();
        cache.add_entry(entry2).unwrap();

        let entries = cache.get_entries("hash123").unwrap();
        assert_eq!(entries.len(), 2);

        let tool_names: Vec<_> = entries.iter().map(|e| &e.tool_name).collect();
        assert!(tool_names.contains(&&"tool1".to_string()));
        assert!(tool_names.contains(&&"tool2".to_string()));
    }

    #[tokio::test]
    async fn test_get_latest_analysis() {
        let temp_dir = TempDir::new().unwrap();
        let cache = AnalysisCache::new(temp_dir.path()).unwrap();

        let now = Utc::now();
        let mut entry1 = create_test_entry("hash123", "test_tool", "/test/file.bin");
        entry1.timestamp = now - Duration::hours(1);

        let mut entry2 = create_test_entry("hash123", "test_tool", "/test/file.bin");
        entry2.timestamp = now;

        cache.add_entry(entry1).unwrap();
        cache.add_entry(entry2).unwrap();

        let latest = cache.get_latest_analysis("hash123", "test_tool").unwrap();
        assert_eq!(latest.timestamp, now);
    }

    #[tokio::test]
    async fn test_get_latest_analysis_different_tools() {
        let temp_dir = TempDir::new().unwrap();
        let cache = AnalysisCache::new(temp_dir.path()).unwrap();

        let entry1 = create_test_entry("hash123", "tool1", "/test/file.bin");
        let entry2 = create_test_entry("hash123", "tool2", "/test/file.bin");

        cache.add_entry(entry1).unwrap();
        cache.add_entry(entry2).unwrap();

        let latest_tool1 = cache.get_latest_analysis("hash123", "tool1");
        let latest_tool2 = cache.get_latest_analysis("hash123", "tool2");
        let latest_nonexistent = cache.get_latest_analysis("hash123", "tool3");

        assert!(latest_tool1.is_some());
        assert!(latest_tool2.is_some());
        assert!(latest_nonexistent.is_none());
    }

    #[tokio::test]
    async fn test_get_all_entries() {
        let temp_dir = TempDir::new().unwrap();
        let cache = AnalysisCache::new(temp_dir.path()).unwrap();

        let entry1 = create_test_entry("hash1", "tool1", "/test/file1.bin");
        let entry2 = create_test_entry("hash2", "tool2", "/test/file2.bin");
        let entry3 = create_test_entry("hash1", "tool2", "/test/file1.bin");

        cache.add_entry(entry1).unwrap();
        cache.add_entry(entry2).unwrap();
        cache.add_entry(entry3).unwrap();

        let all_entries = cache.get_all_entries();
        assert_eq!(all_entries.len(), 3);
    }

    #[tokio::test]
    async fn test_get_metadata() {
        let temp_dir = TempDir::new().unwrap();
        let cache = AnalysisCache::new(temp_dir.path()).unwrap();

        let entry1 = create_test_entry("hash1", "tool1", "/test/file1.bin");
        let entry2 = create_test_entry("hash2", "tool2", "/test/file2.bin");

        cache.add_entry(entry1).unwrap();
        cache.add_entry(entry2).unwrap();

        let metadata = cache.get_metadata();
        assert_eq!(metadata.total_entries, 2);
        assert_eq!(metadata.total_unique_files, 2);
        assert!(metadata.cache_size_bytes > 0);
    }

    #[tokio::test]
    async fn test_clear_cache() {
        let temp_dir = TempDir::new().unwrap();
        let cache = AnalysisCache::new(temp_dir.path()).unwrap();

        let entry = create_test_entry("hash123", "test_tool", "/test/file.bin");
        cache.add_entry(entry).unwrap();

        let metadata_before = cache.get_metadata();
        assert_eq!(metadata_before.total_entries, 1);

        cache.clear().unwrap();

        let metadata_after = cache.get_metadata();
        assert_eq!(metadata_after.total_entries, 0);
        assert_eq!(metadata_after.total_unique_files, 0);
    }

    #[tokio::test]
    async fn test_search_by_tool_name() {
        let temp_dir = TempDir::new().unwrap();
        let cache = AnalysisCache::new(temp_dir.path()).unwrap();

        let entry1 = create_test_entry("hash1", "hash_tool", "/test/file1.bin");
        let entry2 = create_test_entry("hash2", "string_tool", "/test/file2.bin");
        let entry3 = create_test_entry("hash3", "hash_tool", "/test/file3.bin");

        cache.add_entry(entry1).unwrap();
        cache.add_entry(entry2).unwrap();
        cache.add_entry(entry3).unwrap();

        let query = CacheSearchQuery {
            tool_name: Some("hash_tool".to_string()),
            file_path_pattern: None,
            start_time: None,
            end_time: None,
            min_file_size: None,
            max_file_size: None,
        };

        let results = cache.search_entries(&query);
        assert_eq!(results.len(), 2);
        assert!(results.iter().all(|e| e.tool_name == "hash_tool"));
    }

    #[tokio::test]
    async fn test_search_by_file_path_pattern() {
        let temp_dir = TempDir::new().unwrap();
        let cache = AnalysisCache::new(temp_dir.path()).unwrap();

        let entry1 = create_test_entry("hash1", "tool", "/bin/ls");
        let entry2 = create_test_entry("hash2", "tool", "/usr/bin/cat");
        let entry3 = create_test_entry("hash3", "tool", "/home/user/file.txt");

        cache.add_entry(entry1).unwrap();
        cache.add_entry(entry2).unwrap();
        cache.add_entry(entry3).unwrap();

        let query = CacheSearchQuery {
            tool_name: None,
            file_path_pattern: Some("bin".to_string()),
            start_time: None,
            end_time: None,
            min_file_size: None,
            max_file_size: None,
        };

        let results = cache.search_entries(&query);
        assert_eq!(results.len(), 2);
        assert!(results.iter().all(|e| e.file_path.contains("bin")));
    }

    #[tokio::test]
    async fn test_search_by_time_range() {
        let temp_dir = TempDir::new().unwrap();
        let cache = AnalysisCache::new(temp_dir.path()).unwrap();

        let now = Utc::now();
        let hour_ago = now - Duration::hours(1);
        let two_hours_ago = now - Duration::hours(2);

        let mut entry1 = create_test_entry("hash1", "tool", "/test/file1.bin");
        entry1.timestamp = two_hours_ago;

        let mut entry2 = create_test_entry("hash2", "tool", "/test/file2.bin");
        entry2.timestamp = hour_ago;

        let mut entry3 = create_test_entry("hash3", "tool", "/test/file3.bin");
        entry3.timestamp = now;

        cache.add_entry(entry1).unwrap();
        cache.add_entry(entry2).unwrap();
        cache.add_entry(entry3).unwrap();

        let query = CacheSearchQuery {
            tool_name: None,
            file_path_pattern: None,
            start_time: Some(hour_ago - Duration::minutes(30)),
            end_time: Some(now + Duration::minutes(30)),
            min_file_size: None,
            max_file_size: None,
        };

        let results = cache.search_entries(&query);
        assert_eq!(results.len(), 2); // Should exclude the entry from 2 hours ago
    }

    #[tokio::test]
    async fn test_search_by_file_size() {
        let temp_dir = TempDir::new().unwrap();
        let cache = AnalysisCache::new(temp_dir.path()).unwrap();

        let mut entry1 = create_test_entry("hash1", "tool", "/test/small.bin");
        entry1.file_size = 100;

        let mut entry2 = create_test_entry("hash2", "tool", "/test/medium.bin");
        entry2.file_size = 1000;

        let mut entry3 = create_test_entry("hash3", "tool", "/test/large.bin");
        entry3.file_size = 10000;

        cache.add_entry(entry1).unwrap();
        cache.add_entry(entry2).unwrap();
        cache.add_entry(entry3).unwrap();

        let query = CacheSearchQuery {
            tool_name: None,
            file_path_pattern: None,
            start_time: None,
            end_time: None,
            min_file_size: Some(500),
            max_file_size: Some(5000),
        };

        let results = cache.search_entries(&query);
        assert_eq!(results.len(), 1);
        assert_eq!(results[0].file_size, 1000);
    }

    #[tokio::test]
    async fn test_search_combined_criteria() {
        let temp_dir = TempDir::new().unwrap();
        let cache = AnalysisCache::new(temp_dir.path()).unwrap();

        let entry1 = create_test_entry("hash1", "hash_tool", "/bin/ls");
        let entry2 = create_test_entry("hash2", "string_tool", "/bin/cat");
        let entry3 = create_test_entry("hash3", "hash_tool", "/usr/bin/grep");

        cache.add_entry(entry1).unwrap();
        cache.add_entry(entry2).unwrap();
        cache.add_entry(entry3).unwrap();

        let query = CacheSearchQuery {
            tool_name: Some("hash_tool".to_string()),
            file_path_pattern: Some("bin".to_string()),
            start_time: None,
            end_time: None,
            min_file_size: None,
            max_file_size: None,
        };

        let results = cache.search_entries(&query);
        assert_eq!(results.len(), 2);
        assert!(results.iter().all(|e| e.tool_name == "hash_tool" && e.file_path.contains("bin")));
    }

    #[tokio::test]
    async fn test_get_statistics() {
        let temp_dir = TempDir::new().unwrap();
        let cache = AnalysisCache::new(temp_dir.path()).unwrap();

        let mut entry1 = create_test_entry("hash1", "hash_tool", "/test/file.exe");
        entry1.execution_time_ms = 100;

        let mut entry2 = create_test_entry("hash2", "string_tool", "/test/file.dll");
        entry2.execution_time_ms = 200;

        let mut entry3 = create_test_entry("hash3", "hash_tool", "/test/file.txt");
        entry3.execution_time_ms = 150;

        cache.add_entry(entry1).unwrap();
        cache.add_entry(entry2).unwrap();
        cache.add_entry(entry3).unwrap();

        let stats = cache.get_statistics();
        assert_eq!(stats.total_analyses, 3);
        assert_eq!(stats.unique_files, 3);
        assert_eq!(stats.avg_execution_time_ms, 150); // (100 + 200 + 150) / 3

        assert_eq!(stats.tool_counts["hash_tool"], 2);
        assert_eq!(stats.tool_counts["string_tool"], 1);

        assert_eq!(stats.file_type_counts["exe"], 1);
        assert_eq!(stats.file_type_counts["dll"], 1);
        assert_eq!(stats.file_type_counts["txt"], 1);
    }

    #[tokio::test]
    async fn test_max_entries_per_file_limit() {
        let temp_dir = TempDir::new().unwrap();
        let mut cache = AnalysisCache::new(temp_dir.path()).unwrap();
        cache.max_entries_per_file = 3; // Set a low limit for testing

        // Add more entries than the limit
        for i in 0..5 {
            let entry = create_test_entry("hash123", &format!("tool{}", i), "/test/file.bin");
            cache.add_entry(entry).unwrap();
        }

        let entries = cache.get_entries("hash123").unwrap();
        assert_eq!(entries.len(), 3); // Should be limited
    }

    #[tokio::test]
    async fn test_cache_entry_serialization() {
        let entry = CacheEntry {
            file_path: "/test/file.bin".to_string(),
            file_hash: "abc123".to_string(),
            tool_name: "test_tool".to_string(),
            tool_args: [("arg1".to_string(), serde_json::json!("value1"))].iter().cloned().collect(),
            result: serde_json::json!({"result": "success"}),
            timestamp: Utc::now(),
            file_size: 2048,
            execution_time_ms: 250,
        };

        // Test JSON serialization
        let json = serde_json::to_string(&entry).unwrap();
        let deserialized: CacheEntry = serde_json::from_str(&json).unwrap();

        assert_eq!(deserialized.file_path, entry.file_path);
        assert_eq!(deserialized.file_hash, entry.file_hash);
        assert_eq!(deserialized.tool_name, entry.tool_name);
        assert_eq!(deserialized.file_size, entry.file_size);
        assert_eq!(deserialized.execution_time_ms, entry.execution_time_ms);
    }

    #[tokio::test]
    async fn test_cache_metadata_serialization() {
        let metadata = CacheMetadata {
            total_entries: 10,
            total_unique_files: 5,
            cache_size_bytes: 1024,
            last_updated: Utc::now(),
        };

        // Test JSON serialization
        let json = serde_json::to_string(&metadata).unwrap();
        let deserialized: CacheMetadata = serde_json::from_str(&json).unwrap();

        assert_eq!(deserialized.total_entries, metadata.total_entries);
        assert_eq!(deserialized.total_unique_files, metadata.total_unique_files);
        assert_eq!(deserialized.cache_size_bytes, metadata.cache_size_bytes);
    }

    #[tokio::test]
    async fn test_cache_search_query_serialization() {
        let query = CacheSearchQuery {
            tool_name: Some("test_tool".to_string()),
            file_path_pattern: Some("/test/".to_string()),
            start_time: Some(Utc::now()),
            end_time: Some(Utc::now()),
            min_file_size: Some(100),
            max_file_size: Some(1000),
        };

        // Test JSON serialization
        let json = serde_json::to_string(&query).unwrap();
        let deserialized: CacheSearchQuery = serde_json::from_str(&json).unwrap();

        assert_eq!(deserialized.tool_name, query.tool_name);
        assert_eq!(deserialized.file_path_pattern, query.file_path_pattern);
        assert_eq!(deserialized.min_file_size, query.min_file_size);
        assert_eq!(deserialized.max_file_size, query.max_file_size);
    }

    #[tokio::test]
    async fn test_cache_statistics_serialization() {
        let stats = CacheStatistics {
            tool_counts: [("tool1".to_string(), 5), ("tool2".to_string(), 3)].iter().cloned().collect(),
            file_type_counts: [("exe".to_string(), 2), ("dll".to_string(), 1)].iter().cloned().collect(),
            total_analyses: 8,
            unique_files: 6,
            avg_execution_time_ms: 150,
        };

        // Test JSON serialization
        let json = serde_json::to_string(&stats).unwrap();
        let deserialized: CacheStatistics = serde_json::from_str(&json).unwrap();

        assert_eq!(deserialized.total_analyses, stats.total_analyses);
        assert_eq!(deserialized.unique_files, stats.unique_files);
        assert_eq!(deserialized.avg_execution_time_ms, stats.avg_execution_time_ms);
        assert_eq!(deserialized.tool_counts, stats.tool_counts);
        assert_eq!(deserialized.file_type_counts, stats.file_type_counts);
    }

    #[tokio::test]
    async fn test_empty_cache_operations() {
        let temp_dir = TempDir::new().unwrap();
        let cache = AnalysisCache::new(temp_dir.path()).unwrap();

        // Test operations on empty cache
        assert!(cache.get_entries("nonexistent").is_none());
        assert!(cache.get_latest_analysis("nonexistent", "tool").is_none());

        let all_entries = cache.get_all_entries();
        assert!(all_entries.is_empty());

        let empty_query = CacheSearchQuery {
            tool_name: None,
            file_path_pattern: None,
            start_time: None,
            end_time: None,
            min_file_size: None,
            max_file_size: None,
        };

        let results = cache.search_entries(&empty_query);
        assert!(results.is_empty());

        let stats = cache.get_statistics();
        assert_eq!(stats.total_analyses, 0);
        assert_eq!(stats.unique_files, 0);
        assert_eq!(stats.avg_execution_time_ms, 0);
    }

    #[tokio::test]
    async fn test_cache_persistence() {
        let temp_dir = TempDir::new().unwrap();
        
        {
            let cache = AnalysisCache::new(temp_dir.path()).unwrap();
            let entry = create_test_entry("persistent_hash", "test_tool", "/test/file.bin");
            cache.add_entry(entry).unwrap();
            
            // Wait a bit for async save to complete
            tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
        }

        // Create new cache instance to test loading
        let cache2 = AnalysisCache::new(temp_dir.path()).unwrap();
        let entries = cache2.get_entries("persistent_hash");
        
        // May or may not find entries depending on timing of async save
        // This test mainly ensures no errors occur during persistence operations
        assert!(entries.is_some() || entries.is_none());
    }
}
